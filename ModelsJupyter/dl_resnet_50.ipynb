{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_resnet_50.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IPqshKrcVnPl",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0qXNVnk6y-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwCXyxZx6r5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Imports\n",
        "# General\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "keras = tf.keras\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C8lT43tBQL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Network input specs\n",
        "IMG_SIZE  = 224\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "## Hyperparameters\n",
        "NUM_CLASSES = 100\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "ARCHITECTURE = 'ResNet50'\n",
        "NODES_HIDDEN_0 = 512\n",
        "BASE_TRAINABLE = False\n",
        "REGULARIZER = 'l2' # 'None' | 'l1' | 'l2' \n",
        "REGULARIZATZION_STRENGTH = '0.01'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6U3PopMc9tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For documentation purposes - Add all parameters set above to this dict\n",
        "params = dict(\n",
        "    img_size = IMG_SIZE,\n",
        "    img_shape = (IMG_SIZE, IMG_SIZE, 3),\n",
        "    num_classes = NUM_CLASSES,\n",
        "    epochs = EPOCHS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    architecture = ARCHITECTURE,\n",
        "    nodes_hidden_0 = NODES_HIDDEN_0,\n",
        "    base_trainable = BASE_TRAINABLE,\n",
        "    regularizer = REGULARIZER,\n",
        "    regularization_strength = REGULARIZATZION_STRENGTH,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FX_Ph-WXUbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Build model components from string-specifications\n",
        "\n",
        "architecture = eval('tf.keras.applications.' + ARCHITECTURE)\n",
        "regularizer = None if REGULARIZER is 'None' else eval('regularizers.' + REGULARIZER + '(' + REGULARIZATZION_STRENGTH + ')')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM_CeXXFKXcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Set path for saving training progress & data\n",
        "now = datetime.now()\n",
        "TIME_STAMP = now.strftime(\"_%Y_%d_%m__%H_%M_%S__%f\")\n",
        "MODEL_ID = 'Model_' + TIME_STAMP + '/'\n",
        "\n",
        "DATA_STORAGE_PATH = '/content/drive/My Drive/DL/P1_experiments/'\n",
        "TRAINED_MODELS = 'Trained_Models/'\n",
        "MODEL_ARCHITECTURE = ARCHITECTURE + '/'\n",
        "path = DATA_STORAGE_PATH + TRAINED_MODELS + MODEL_ARCHITECTURE + MODEL_ID\n",
        "\n",
        "## Create folder\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "    print('Created dir: ' + path)\n",
        "else:\n",
        "    path = None\n",
        "    raise Exception('PATH EXISTS!')\n",
        "\n",
        "## Save settings used during model run\n",
        "\n",
        "# Save as csv - nicer to read for human\n",
        "with open(path+'params.csv', 'w') as f:\n",
        "    for key in params.keys():\n",
        "        f.write(\"%s,%s\\n\"%(key,params[key]))\n",
        "f.close()\n",
        "\n",
        "# Save as json - easier to import again later\n",
        "with open(path+'params.json', 'w') as f:\n",
        "    json.dump(params, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCNT44_b60Vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Obtain dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
        "\n",
        "# Normalize\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtykwxpVAXOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Data Generators setup\n",
        "\n",
        "# For data generators\n",
        "\n",
        "def preprocessing_function(x):\n",
        "    \"\"\"\n",
        "      Can be used for data augmentation.\n",
        "    \"\"\"\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gViw_Y6x8udt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training generator\n",
        "\n",
        "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
        "train_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(#rescale=1./255, # Done already \n",
        "                                                     #preprocessing_function=preprocessing_function  # Pre-processing function may be passed here\n",
        "                                                     )\n",
        "\n",
        "train_data_gen = train_image_generator.flow(x_train, \n",
        "                                            y_train,\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr10MCgeJTUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test generator\n",
        "\n",
        "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
        "test_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(#rescale=1./255, # Done already \n",
        "                                                     #preprocessing_function=preprocessing_function  # Pre-processing function may be passed here\n",
        "                                                     )\n",
        "\n",
        "test_data_gen = test_image_generator.flow(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA2PPmPgKBiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Definition of callbacks adjusted from https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "        monitor='val_accuracy',    # Stop training when `val_loss` is no longer improving\n",
        "        min_delta=0,               # \"no longer improving\" being defined as \"no better than 0|5e-1 less\"\n",
        "        patience=2,                # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "        verbose=0)                 # Quantity of printed output\n",
        "\n",
        "model_saving_callback = ModelCheckpoint(\n",
        "        filepath=path+'best_model.h5',\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_loss` score has improved.\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        # mode: one of {auto, min, max}. If `save_best_only=True`, the decision to\n",
        "        # overwrite the current save file is made based on either the maximization\n",
        "        # or the minimization of the monitored quantity. For `val_acc`, this\n",
        "        # should be `max`, for `val_loss` this should be `min`, etc. In `auto`\n",
        "        # mode, the direction is automatically inferred from the name of the\n",
        "        # monitored quantity.\n",
        "        verbose=0)\n",
        "\n",
        "# Join list of required callbacks\n",
        "callbacks = [model_saving_callback] # , early_stopping_callback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YUlN1WjNQ0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Helper Layer\n",
        "\n",
        "# Resizing from (32,32,3) to (224,224,3)\n",
        "\n",
        "class Resizer(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(Resizer, self).__init__()\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "  \n",
        "  def call(self, input):\n",
        "    return tf.image.resize(input, (IMG_SIZE, IMG_SIZE))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8F7MoDf7KJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Obtain & Compile Model\n",
        "\n",
        "# Reset tf sessions\n",
        "tf.keras.backend.clear_session()  # Destroys the current TF graph and creates a new one.\n",
        "\n",
        "# Create the base model from the pre-trained model specified by variable ARCHITECTURE\n",
        "base_model = architecture(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)\n",
        "\n",
        "base_model.trainable = BASE_TRAINABLE\n",
        "\n",
        "# Print summary base model\n",
        "#print('Base model:')\n",
        "#base_model.summary()\n",
        "\n",
        "#global_average_layer = tf.keras.layers.GlobalAveragePooling2D()  # Suggested on TF tutorial page... \n",
        "flatten_operation = layers.Flatten()\n",
        "hidden_dense_layer_0 = layers.Dense(NODES_HIDDEN_0, activation='relu', kernel_regularizer=regularizer)\n",
        "prediction_layer = layers.Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizer)\n",
        "\n",
        "# Construct overall model\n",
        "model = tf.keras.Sequential([\n",
        "  Resizer(),\n",
        "  base_model,\n",
        "  #global_average_layer,\n",
        "  flatten_operation,\n",
        "  hidden_dense_layer_0,\n",
        "  prediction_layer\n",
        "])\n",
        "\n",
        "# Compile model & make some design choices\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001,\n",
        "                                           beta_1=0.9,\n",
        "                                           beta_2=0.999,\n",
        "                                           epsilon=1e-07,\n",
        "                                           amsgrad=False,\n",
        "                                           name='Adam'\n",
        "                                           ),\n",
        "              loss='sparse_categorical_crossentropy',  # Capable of working with regularization\n",
        "              metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
        "\n",
        "\n",
        "# Construct computational graph with proper dimensions\n",
        "inputs = np.random.random([1] + list(IMG_SHAPE)).astype(np.float32)\n",
        "model(inputs)\n",
        "\n",
        "# Print summary overall model\n",
        "print('Overall model:')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmtysFNxIyAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Perform training\n",
        "\n",
        "history = model.fit(\n",
        "                    x=train_data_gen,\n",
        "                    #y=None,\n",
        "                    #batch_size=None,\n",
        "                    epochs=EPOCHS,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks,\n",
        "                    #validation_split=0.0,\n",
        "                    validation_data=test_data_gen,\n",
        "                    #shuffle=True,\n",
        "                    #class_weight=None,\n",
        "                    #sample_weight=None,\n",
        "                    initial_epoch=0,\n",
        "                    steps_per_epoch=20,\n",
        "                    #validation_steps=18,\n",
        "                    #validation_freq=1,\n",
        "                    #max_queue_size=5,\n",
        "                    #workers=1,\n",
        "                    #use_multiprocessing=False,\n",
        "                    #**kwargs\n",
        "                    )\n",
        "\n",
        "# Save the entire model as a final model to a HDF5 file.\n",
        "name = 'final_model'\n",
        "model.save(path+name+'.h5')\n",
        "\n",
        "# Record training progress\n",
        "with open(path+'training_progress.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\", \"sparse_categorical_crossentropy\"])\n",
        "    for line in range(len(history.history['loss'])): \n",
        "        epoch = str(line+1)\n",
        "        writer.writerow([epoch,\n",
        "                         history.history[\"loss\"][line], \n",
        "                         history.history[\"accuracy\"][line], \n",
        "                         history.history[\"val_loss\"][line], \n",
        "                         history.history[\"val_accuracy\"][line], \n",
        "                         history.history[\"sparse_categorical_crossentropy\"][line]\n",
        "                         ])\n",
        "    # Save some more important bits/summary\n",
        "    writer.writerow([\"End of training. Summary:\"])\n",
        "    writer.writerow([\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\", \"sparse_categorical_crossentropy\"])\n",
        "    # Max accuracy\n",
        "    writer.writerow([\"Max accuracy row:\"])\n",
        "    x = np.argmax(history.history[\"accuracy\"])\n",
        "    writer.writerow([str(x+1),\n",
        "                         history.history[\"loss\"][x], \n",
        "                         history.history[\"accuracy\"][x], \n",
        "                         history.history[\"val_loss\"][x], \n",
        "                         history.history[\"val_accuracy\"][x], \n",
        "                         history.history[\"sparse_categorical_crossentropy\"][x]\n",
        "                         ])\n",
        "    # Max val_accuracy\n",
        "    writer.writerow([\"Max val_accuracy row:\"])\n",
        "    x = np.argmax(history.history[\"val_accuracy\"])\n",
        "    writer.writerow([str(x+1),\n",
        "                         history.history[\"loss\"][x], \n",
        "                         history.history[\"accuracy\"][x], \n",
        "                         history.history[\"val_loss\"][x], \n",
        "                         history.history[\"val_accuracy\"][x], \n",
        "                         history.history[\"sparse_categorical_crossentropy\"][x]\n",
        "                         ])\n",
        "    file.close()\n",
        "\n",
        "print('Done.')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}