{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 29s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"dl_resnet_50.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1y2-qOT5RECsZjbknJ26ivPwL_-qOD_OC\n",
    "\"\"\"\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %tensorflow_version 2.x\n",
    "!pip install --upgrade tensorflow\n",
    "!pip install --upgrade keras\n",
    "'''\n",
    "\n",
    "\n",
    "## Imports\n",
    "# General\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "'''\n",
    "print(tf.__version__)\n",
    "config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "sess = tf.compat.v1.Session(config=config) \n",
    "\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "'''\n",
    "\n",
    "\n",
    "## Network input specs\n",
    "IMG_SIZE  = 224\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "## Hyperparameters\n",
    "NUM_CLASSES = 100\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 32\n",
    "#ARCHITECTURE = 'ResNet50'\n",
    "ARCHITECTURE ='InceptionV3'\n",
    "NODES_HIDDEN_0 =512\n",
    "NODES_HIDDEN_1 = 512\n",
    "BASE_TRAINABLE = True\n",
    "REGULARIZER = 'l2' # 'None' | 'l1' | 'l2' \n",
    "REGULARIZATZION_STRENGTH = '0.01'\n",
    "AUGMENTATION = 1\n",
    "\n",
    "## For documentation purposes - Add all parameters set above to this dict\n",
    "params = dict(\n",
    "    img_size = IMG_SIZE,\n",
    "    img_shape = (IMG_SIZE, IMG_SIZE, 3),\n",
    "    num_classes = NUM_CLASSES,\n",
    "    epochs = EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    architecture = ARCHITECTURE,\n",
    "    nodes_hidden_0 = NODES_HIDDEN_0,\n",
    "    #nodes_hidden_1 = NODES_HIDDEN_1,\n",
    "    base_trainable = BASE_TRAINABLE,\n",
    "    regularizer = REGULARIZER,\n",
    "    augmentation = AUGMENTATION,\n",
    "    regularization_strength = REGULARIZATZION_STRENGTH,\n",
    ")\n",
    "\n",
    "## Build model components from string-specifications\n",
    "\n",
    "architecture = eval('tf.keras.applications.' + ARCHITECTURE)\n",
    "regularizer = None if REGULARIZER is 'None' else eval('regularizers.' + REGULARIZER + '(' + REGULARIZATZION_STRENGTH + ')')\n",
    "\n",
    "## Set path for saving training progress & data\n",
    "now = datetime.now()\n",
    "TIME_STAMP = now.strftime(\"_%Y_%d_%m__%H_%M_%S__%f\")\n",
    "MODEL_ID = 'Model_' + TIME_STAMP + '/'\n",
    "\n",
    "#DATA_STORAGE_PATH = '/data/s3993914/Dl_output/'\n",
    "TRAINED_MODELS = 'Trained_Models/'\n",
    "MODEL_ARCHITECTURE = ARCHITECTURE + '/'\n",
    "#path = DATA_STORAGE_PATH + TRAINED_MODELS + MODEL_ARCHITECTURE + MODEL_ID\n",
    "\n",
    "## Create folder\n",
    "'''\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    print('Created dir: ' + path)\n",
    "else:\n",
    "    path = None\n",
    "    raise Exception('PATH EXISTS!')\n",
    "\n",
    "## Save settings used during model run\n",
    "\n",
    "# Save as csv - nicer to read for human\n",
    "with open(path+'params.csv', 'w') as f:\n",
    "    for key in params.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,params[key]))\n",
    "f.close()\n",
    "\n",
    "# Save as json - easier to import again later\n",
    "with open(path+'params.json', 'w') as f:\n",
    "    json.dump(params, f)\n",
    "f.close()\n",
    "'''\n",
    "## Obtain dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "# Normalize\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "## Data Generators setup\n",
    "\n",
    "# For data generators\n",
    "\n",
    "def preprocessing_function(x):\n",
    "    \"\"\"\n",
    "      Can be used for data augmentation.\n",
    "    \"\"\"\n",
    "    if (AUGMENTATION):\n",
    "        datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        )\n",
    "        x=datagen.fit(x_train) \n",
    "        print(x.shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Training generator\n",
    "\n",
    "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
    "train_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(#rescale=1./255, # Done already \n",
    "                                                     preprocessing_function=preprocessing_function  # Pre-processing function may be passed here\n",
    "                                                     )\n",
    "\n",
    "train_data_gen = train_image_generator.flow(x_train, \n",
    "                                            y_train,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True)\n",
    "\n",
    "# Test generator\n",
    "\n",
    "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(#rescale=1./255, # Done already \n",
    "                                                     #preprocessing_function=preprocessing_function  # Pre-processing function may be passed here\n",
    "                                                     )\n",
    "\n",
    "test_data_gen = test_image_generator.flow(x_test, y_test)\n",
    "\n",
    "## Definition of callbacks adjusted from https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_accuracy',    # Stop training when `val_loss` is no longer improving\n",
    "        min_delta=0,               # \"no longer improving\" being defined as \"no better than 0|5e-1 less\"\n",
    "        patience=2,                # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        verbose=0)                 # Quantity of printed output\n",
    "\n",
    "model_saving_callback = ModelCheckpoint(\n",
    "        filepath=path+'best_model.h5',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        # mode: one of {auto, min, max}. If `save_best_only=True`, the decision to\n",
    "        # overwrite the current save file is made based on either the maximization\n",
    "        # or the minimization of the monitored quantity. For `val_acc`, this\n",
    "        # should be `max`, for `val_loss` this should be `min`, etc. In `auto`\n",
    "        # mode, the direction is automatically inferred from the name of the\n",
    "        # monitored quantity.\n",
    "        verbose=0)\n",
    "\n",
    "# Join list of required callbacks\n",
    "callbacks = [model_saving_callback] # , early_stopping_callback\n",
    "\n",
    "## Helper Layer\n",
    "\n",
    "# Resizing from (32,32,3) to (224,224,3)\n",
    "\n",
    "class Resizer(layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(Resizer, self).__init__()\n",
    "\n",
    "  def build(self, input_shapes):\n",
    "    pass\n",
    "  \n",
    "  def call(self, input):\n",
    "    return tf.image.resize(input, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "## Obtain & Compile Model\n",
    "\n",
    "# Reset tf sessions\n",
    "tf.keras.backend.clear_session()  # Destroys the current TF graph and creates a new one.\n",
    "\n",
    "# Create the base model from the pre-trained model specified by variable ARCHITECTURE\n",
    "#Include Top = false \n",
    "base_model = architecture(weights=\"imagenet\", include_top=False, input_shape=IMG_SHAPE)\n",
    "\n",
    "base_model.trainable = BASE_TRAINABLE\n",
    "\n",
    "# Print summary base model\n",
    "#print('Base model:')\n",
    "#base_model.summary()\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()  # Suggested on TF tutorial page... \n",
    "flatten_operation = layers.Flatten()\n",
    "hidden_dense_layer_0 = layers.Dense(NODES_HIDDEN_0, activation='relu', kernel_regularizer=regularizer)\n",
    "hidden_dense_layer_1 = layers.Dense(NODES_HIDDEN_1, activation='relu', kernel_regularizer=regularizer)\n",
    "prediction_layer = layers.Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizer)\n",
    "\n",
    "# Construct overall model\n",
    "model = tf.keras.Sequential([\n",
    "  Resizer(),\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  flatten_operation,\n",
    "  hidden_dense_layer_0,\n",
    "  prediction_layer\n",
    "])\n",
    "\n",
    "# Compile model & make some design choices\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001,\n",
    "                                           beta_1=0.9,\n",
    "                                           beta_2=0.999,\n",
    "                                           epsilon=1e-07,\n",
    "                                           amsgrad=False,\n",
    "                                           name='Adam'\n",
    "                                           ),\n",
    "              loss='sparse_categorical_crossentropy',  # Capable of working with regularization\n",
    "              metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "\n",
    "\n",
    "# Construct computational graph with proper dimensions\n",
    "inputs = np.random.random([1] + list(IMG_SHAPE)).astype(np.float32)\n",
    "model(inputs)\n",
    "\n",
    "# Print summary overall model\n",
    "print('Overall model:')\n",
    "model.summary()\n",
    "\n",
    "## Perform training\n",
    "\n",
    "history = model.fit(\n",
    "                    x=train_data_gen,\n",
    "                    #y=None,\n",
    "                    #batch_size=None,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    #validation_split=0.0,\n",
    "                    validation_data=test_data_gen,\n",
    "                    #shuffle=True,\n",
    "                    #class_weight=None,\n",
    "                    #sample_weight=None,\n",
    "                    initial_epoch=0,\n",
    "                    steps_per_epoch=20,\n",
    "                    #validation_steps=18,\n",
    "                    #validation_freq=1,\n",
    "                    #max_queue_size=5,\n",
    "                    #workers=1,\n",
    "                    #use_multiprocessing=False,\n",
    "                    #**kwargs\n",
    "                    )\n",
    "\n",
    "# Save the entire model as a final model to a HDF5 file.\n",
    "name = 'final_model'\n",
    "model.save(path+name+'.h5')\n",
    "\n",
    "# Record training progress\n",
    "with open(path+'training_progress.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\", \"sparse_categorical_crossentropy\"])\n",
    "    for line in range(len(history.history['loss'])): \n",
    "        epoch = str(line+1)\n",
    "        writer.writerow([epoch,\n",
    "                         history.history[\"loss\"][line], \n",
    "                         history.history[\"accuracy\"][line], \n",
    "                         history.history[\"val_loss\"][line], \n",
    "                         history.history[\"val_accuracy\"][line], \n",
    "                         history.history[\"sparse_categorical_crossentropy\"][line]\n",
    "                         ])\n",
    "    # Save some more important bits/summary\n",
    "    writer.writerow([\"End of training. Summary:\"])\n",
    "    writer.writerow([\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\", \"sparse_categorical_crossentropy\"])\n",
    "    # Max accuracy\n",
    "    writer.writerow([\"Max accuracy row:\"])\n",
    "    x = np.argmax(history.history[\"accuracy\"])\n",
    "    writer.writerow([str(x+1),\n",
    "                         history.history[\"loss\"][x], \n",
    "                         history.history[\"accuracy\"][x], \n",
    "                         history.history[\"val_loss\"][x], \n",
    "                         history.history[\"val_accuracy\"][x], \n",
    "                         history.history[\"sparse_categorical_crossentropy\"][x]\n",
    "                         ])\n",
    "    # Max val_accuracy\n",
    "    writer.writerow([\"Max val_accuracy row:\"])\n",
    "    x = np.argmax(history.history[\"val_accuracy\"])\n",
    "    writer.writerow([str(x+1),\n",
    "                         history.history[\"loss\"][x], \n",
    "                         history.history[\"accuracy\"][x], \n",
    "                         history.history[\"val_loss\"][x], \n",
    "                         history.history[\"val_accuracy\"][x], \n",
    "                         history.history[\"sparse_categorical_crossentropy\"][x]\n",
    "                         ])\n",
    "    file.close()\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
